{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Acquisition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import statsmodels.api as sm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv('Walmart.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a282ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample Data:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ef495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning/Preprocessing\n",
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')  \n",
    "\n",
    "# Extract useful time-based features\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482de273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates if any\n",
    "data.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values \n",
    "# Fill missing values using forward fill (or other methods)\n",
    "data.fillna(method='ffill', inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53563c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "walmart_data = pd.read_csv(\"Walmart.csv\")  # Make sure this file exists\n",
    "\n",
    "# Identify categorical and numerical columns dynamically\n",
    "categorical_cols = walmart_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = walmart_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Print identified columns\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b613bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Walmart.csv\")  \n",
    "\n",
    "# Convert Date column to datetime format (if not already converted)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "if 'Weekly_Sales' in df.columns:\n",
    "    X = df.drop(columns=['Weekly_Sales', 'Date'])  # Remove target & date\n",
    "    y = df['Weekly_Sales']\n",
    "\n",
    "    # Apply Time Series Split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Print dataset split details\n",
    "    print(\"\\nDataset Splits:\")\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "else:\n",
    "    print(\"Error: 'Weekly_Sales' column not found in dataset!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Extract date-related features\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['IsHoliday'] = data['Holiday_Flag']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dacc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-fill Weekly_Sales as it's a time-series data\n",
    "data['Weekly_Sales'].fillna(method='ffill', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lag Features\n",
    "# Create lag features for Weekly Sales\n",
    "for lag in [1, 2, 3, 4]:\n",
    "    data[f'Lag_{lag}'] = data['Weekly_Sales'].shift(lag)\n",
    "\n",
    "# Drop missing values after creating lag features\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Features and Target\n",
    "X = data.drop(columns=['Weekly_Sales', 'Date'])  # Dropping 'Date' as it's not used in the model directly\n",
    "y = data['Weekly_Sales']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Specific Columns\n",
    "def standardize_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    return df\n",
    "\n",
    "numerical_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "data = standardize_columns(data, numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using Z-score\n",
    "def remove_outliers_zscore(df, columns, threshold=3):\n",
    "    for col in columns:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        df = df[z_scores < threshold]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f012d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "data = remove_outliers_zscore(data, numerical_cols)\n",
    "print(\"\\nSummary Statistics:\\n\", data.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97343f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "print(\"\\nMean:\\n\", data.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Median\n",
    "print(\"\\nMedian:\\n\", data.median(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda35633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "print(\"\\nStandard Deviation:\\n\", data.std(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "# Histograms\n",
    "data.hist(bins=20, figsize=(12, 8))\n",
    "plt.suptitle(\"Feature Distributions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c767564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c078c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions\n",
    "features_to_plot = ['Temperature', 'Fuel_Price', 'Unemployment']\n",
    "features_to_plot = [col for col in features_to_plot if col in walmart_data.columns]\n",
    "if features_to_plot:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, feature in enumerate(features_to_plot, 1):\n",
    "        plt.subplot(1, len(features_to_plot), i)\n",
    "        sns.histplot(walmart_data[feature], kde=True, bins=30)\n",
    "        plt.title(f'{feature} Distribution')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the correct column (replace 'Store' with the actual column name if different)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data.drop(columns='Store'))  # Adjust 'Store' to the correct column name\n",
    "plt.title(\"Box Plot of Numerical Features\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54876ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  # Import model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # Ensure X_train and y_train are defined\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Ensure y_test is a Pandas Series (avoid shape issues)\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "# Calculate residuals (difference between actual and predicted values)\n",
    "residuals = y_test - predictions\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=predictions, y=residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')  \n",
    "plt.xlabel('Predicted Sales')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot - Walmart Sales Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Trend Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['Date'], data['Weekly_Sales'], label='Weekly Sales')\n",
    "plt.title('Weekly Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91efbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Weekly Sales\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(walmart_data['Weekly_Sales'], bins=30, kde=True)\n",
    "plt.title('Distribution of Weekly Sales')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc70ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots of Sales by Store\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(x='Store', y='Weekly_Sales', data=walmart_data)\n",
    "plt.title('Sales Distribution by Store')\n",
    "plt.xlabel('Store')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d51e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Decomposition\n",
    "result = sm.tsa.seasonal_decompose(data['Weekly_Sales'], model='additive', period=52)\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22181153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data.index, data['Weekly_Sales'] - data['Weekly_Sales'].mean())\n",
    "plt.title(\"Residual Analysis\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f428c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predictive Analysis\n",
    "# Define features and target\n",
    "X = data[['Store', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year', 'Month', 'WeekOfYear', 'Lag_1', 'Lag_2']]\n",
    "y = data['Weekly_Sales']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b45d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Store', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year', 'Month', 'WeekOfYear']]\n",
    "y = data['Weekly_Sales']\n",
    "\n",
    "# 🔹 Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🔹 Identify Numerical and Categorical Features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# 🔹 Define Preprocessing Pipeline\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_features),\n",
    "        ('cat', cat_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dictionary of models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Training & Evaluation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R² Score: {r2:.2f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4509371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"Missing values in X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"Missing values in y_train:\", y_train.isnull().sum())\n",
    "\n",
    "# 2. Check preprocessor\n",
    "print(\"Preprocessor:\", preprocessor)\n",
    "\n",
    "# 3. Check if GridSearchCV runs a simple model\n",
    "test_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "test_model.fit(X_train, y_train)\n",
    "print(\"Simple model test score:\", test_model.score(X_test, y_test))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 🔹 Define Model Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 🔹 Define Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# 🔹 Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# 🔹 Fit Model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Print Best Parameters and Best Score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R^2 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190131ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to Evaluate Model Performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"MAPE:\", mean_absolute_percentage_error(y_true, y_pred))\n",
    "    print(\"R^2 Score:\", r2_score(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"\\nBest Model Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-Squared Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c19858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=20, random_state=42),\n",
    "    'Support Vector Regression': SVR()\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d15a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate Models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    evaluate_model(y_test, y_pred, name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Hyperparameter Tuning (Random Forest)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "print(\"\\nBest Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "y_pred_rf = grid_search_rf.best_estimator_.predict(X_test_scaled)\n",
    "evaluate_model(y_test, y_pred_rf, \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a423a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30538e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Residual Analysis \n",
    "residuals = y_test - predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.title(\"Residual Distribution (Prediction Errors)\")\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "model = best_model.named_steps['model']\n",
    "\n",
    "# Check if the model has feature importances\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    # Get feature names from preprocessor\n",
    "    feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # Create and sort DataFrame\n",
    "    feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot feature importance\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_imp_df)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 5 key drivers\n",
    "    print(\"\\nKey Drivers of Sales:\")\n",
    "    print(feature_imp_df.head(5).to_string(index=False))\n",
    "else:\n",
    "    print(\"Error: Model does not have 'feature_importances_' attribute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Trend Visualization \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=walmart_data['Date'], y=walmart_data['Weekly_Sales'])\n",
    "plt.title('Weekly Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of Sales Per Store\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=walmart_data['Store'], y=walmart_data['Weekly_Sales'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Sales Distribution Per Store')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4916ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Model Comparison Table\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE', 'MAPE', 'R2 Score'])\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create a temporary DataFrame for the model's results\n",
    "    model_results = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'RMSE': [np.sqrt(mean_squared_error(y_test, y_pred))],\n",
    "        'MAE': [mean_absolute_error(y_test, y_pred)],\n",
    "        'MAPE': [mean_absolute_percentage_error(y_test, y_pred)],\n",
    "        'R2 Score': [r2_score(y_test, y_pred)]\n",
    "    })\n",
    "    \n",
    "    # Concatenate the results\n",
    "    results_df = pd.concat([results_df, model_results], ignore_index=True)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\\n\", results_df.sort_values(by=\"R2 Score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Stacking Regressor\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBRegressor(n_estimators=100, random_state=42))\n",
    "    ],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "stacked_model.fit(X_train_scaled, y_train)\n",
    "y_pred_stacked = stacked_model.predict(X_test_scaled)\n",
    "evaluate_model(y_test, y_pred_stacked, \"Stacking Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c25f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for the Stacking Regressor\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_test, y_test - y_pred_stacked, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Residuals (Error)\")\n",
    "plt.title(\"Residual Plot for Stacking Regressor\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d993bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Enhanced Conclusions\n",
    "print(\"\\nEnhanced Conclusions:\")\n",
    "print(\"1. Stacking Regressor outperformed other models with the best R² score.\")\n",
    "print(\"2. 'Holiday_Flag' and 'Lag Features' were the most impactful predictors.\")\n",
    "print(\"3. Future work: Explore deep learning methods like LSTM for better forecasting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e44bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion/Discussion\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"1. Various models provide a comparative perspective on sales prediction accuracy.\")\n",
    "print(\"2. Feature engineering enhances model performance by capturing temporal patterns.\")\n",
    "print(\"3. Residual and seasonal analysis uncovers patterns and anomalies.\")\n",
    "print(\"4. Future work: Explore advanced forecasting techniques and model interpretability improvements.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e327d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62752d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
